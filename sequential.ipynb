{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.backend import clear_session\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading movie lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1044': 'they do to',\n",
      " 'L1045': 'they do not',\n",
      " 'L869': 'like my fear of wearing pastels',\n",
      " 'L870': 'im kidding you know how sometimes you just become this persona and '\n",
      "         'you dont know how to quit',\n",
      " 'L871': 'no',\n",
      " 'L872': 'okay youre gonna need to learn how to lie',\n",
      " 'L924': 'wow',\n",
      " 'L925': 'lets go',\n",
      " 'L984': 'she okay',\n",
      " 'L985': 'i hope so'}\n",
      "\n",
      "[['L194', 'L195', 'L196', 'L197'],\n",
      " ['L198', 'L199'],\n",
      " ['L200', 'L201', 'L202', 'L203'],\n",
      " ['L204', 'L205', 'L206'],\n",
      " ['L207', 'L208'],\n",
      " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
      " ['L276', 'L277'],\n",
      " ['L280', 'L281'],\n",
      " ['L363', 'L364'],\n",
      " ['L365', 'L366']]\n"
     ]
    }
   ],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "# prepare regex for char filtering\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def clean_sentence(line):\n",
    "    line = line.strip().replace('--', '').replace(\"  \", \" \").replace('\"', \"\")\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    return ' '.join(line)\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_lines.txt', 'r', errors='ignore') as f:\n",
    "    lines_as_list = [row.strip() for row in f.readlines()]\n",
    "\n",
    "\n",
    "lines = {}\n",
    "for line in lines_as_list:\n",
    "    lines[\n",
    "        line.split('+++$+++')[0].strip()\n",
    "    ] = clean_sentence(line.split('+++$+++')[-1])  # clean sentences\n",
    "\n",
    "del lines_as_list\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_conversations.txt', 'r', errors='ignore') as f:\n",
    "    conversations = [row.strip() for row in f.readlines()]\n",
    "\n",
    "# only take id's and convert list as string to list as list\n",
    "conversations = [\n",
    "    conversation.split('+++$+++')[-1].strip().replace('[', '').replace(']', '').replace(\"'\", '').replace(\" \", '').split(',') \n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "pprint({k: lines[k] for k in list(lines)[:10]})\n",
    "print()\n",
    "pprint(conversations[:10])\n",
    "\n",
    "assert len([conversation for conversation in conversations if len(conversation) <=1]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map keys to line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yeah', 'what do you think'],\n",
      " ['two legs nice rack',\n",
      "  'yeah whatever i want you to go out with her',\n",
      "  'sure sparky ill get right on it',\n",
      "  'you just said',\n",
      "  'you need money to take a girl out',\n",
      "  'but youd go out with her if you had the cake'],\n",
      " ['you got it verona i pick up the tab you do the honors',\n",
      "  'youre gonna pay me to take out some girl',\n",
      "  'i cant date her sister until that one gets a boyfriend and thats the catch '\n",
      "  'she doesnt want a boyfriend',\n",
      "  'how much'],\n",
      " ['i cant take a girl like that out on twenty bucks', 'fine thirty'],\n",
      " ['take it or leave it this isnt a negotiation',\n",
      "  'fifty and youve got your man'],\n",
      " ['when i shell out fifty i expect results',\n",
      "  'im on it',\n",
      "  'watching the bitch trash my car doesnt count as a date',\n",
      "  'i got her under control she just acts crazed in public to keep up the '\n",
      "  'image'],\n",
      " ['i just upped my price',\n",
      "  'what',\n",
      "  'a hundred bucks a date',\n",
      "  'forget it',\n",
      "  'forget her sister then'],\n",
      " ['its about time', 'a deals a deal'],\n",
      " ['howd you do it', 'do what', 'get her to act like a human'],\n",
      " ['i dont know dorsey the limothe flowers another hundred for the tux',\n",
      "  'enough with the barbie n ken shit i know']]\n"
     ]
    }
   ],
   "source": [
    "conversations_with_lines = []\n",
    "for conversation in conversations:\n",
    "    conversations_with_lines.append([lines[key] for key in conversation])\n",
    "    \n",
    "pprint(conversations_with_lines[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair those things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
      "       'well i thought wed start with pronunciation if thats okay with you'],\n",
      "      dtype='<U2857')\n",
      "array(['well i thought wed start with pronunciation if thats okay with you',\n",
      "       'not the hacking and gagging and spitting part please'],\n",
      "      dtype='<U2857')\n",
      "array(['not the hacking and gagging and spitting part please',\n",
      "       'okay then how bout we try out some french cuisine saturday night'],\n",
      "      dtype='<U2857')\n",
      "array(['youre asking me out thats so cute whats your name again',\n",
      "       'forget it'], dtype='<U2857')\n",
      "array(['no no its my fault we didnt have a proper introduction',\n",
      "       'cameron'], dtype='<U2857')\n",
      "array(['cameron',\n",
      "       'the thing is cameron im at the mercy of a particularly hideous breed of loser my sister i cant date until she does'],\n",
      "      dtype='<U2857')\n",
      "array(['the thing is cameron im at the mercy of a particularly hideous breed of loser my sister i cant date until she does',\n",
      "       'seems like she could get a date easy enough'], dtype='<U2857')\n",
      "array(['why',\n",
      "       'unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something'],\n",
      "      dtype='<U2857')\n",
      "array(['unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
      "       'thats a shame'], dtype='<U2857')\n",
      "array(['gosh if only we could find kat a boyfriend',\n",
      "       'let me see what i can do'], dtype='<U2857')\n"
     ]
    }
   ],
   "source": [
    "def pair_it(my_list):\n",
    "    pairs = []\n",
    "    for i in range(len(my_list) -1):\n",
    "        pairs.append([my_list[i], my_list[i + 1]])\n",
    "    return pairs\n",
    "\n",
    "paired_conversations_agg = [\n",
    "    pair_it(conversation) for conversation in conversations_with_lines\n",
    "]\n",
    "conversations_pairs = np.array([item for sublist in paired_conversations_agg for item in sublist])\n",
    "for i in range(10):\n",
    "    pprint(conversations_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest conversation: \n",
      "['what'\n",
      " 'then lets begin with the story itself its a story of the grail mythand although there are several variations my favorite begins with the fisher king as a young boy who had to spend a night alone in the forest to prove his courage and during that night he is visited by a sacred vision out of the fire appears the holy grail gods highest symbol of divine grace and a voice says to the boy you shall be the guardian of the grail that it may heal the hearts of menbut the boy was overcome innocent and foolish he was blinded by greater visions a life ahead filled with beauty and glory hope and powertears filled his eyes as he sensed his own invincibility a boys tears of naive wonder and inspiration and in this state ofradical amazementhe felt for a brief moment not like a boy but like god and so he reached into the fire to take the grail and the grail vanished and the boy hands were left caught in the flamesleaving him wounded and ashamed at what his recklessness had lost him when he became king he was determined to reclaim his destiny and find the grail but with each year that passed with each campaign he fought the grail remained lost and this wound he suffered in the fire grew worse he became a bitter man life for him lost its reason with each disappointment with each betrayal with each loss this wound would grow soon the land began to spoil from neglect and his people starveduntil finally the king lost all faith in gods existance and in mans valuehe lost his ability to love or be loved and he was so sick with experience that he started to die as the years went on his bravest knights would search for the grail that would heal their king and make them the most respected and valued men in the land but to no avail pretty soon finding the grail became a ruthless struggle between ambitious men vying for the kings power which only confirmed the kings worst suspicions of man causing his wound to grow his only hope he thought was death then one day a fool was brought in to the king to cheer him he was a simpleminded man not particularly skilledor admired he tells the king some jokessing him some songs but the king feels even worsefinally the fool says what is it that hurts you so much how can i helpand the king says i need a sip of water to cool my throatso the fool takes a cup from the bedstand fills it with water and hands it to the kingsuddenly the king feels a lot better and when he looks to his hands he sees that it was the holy grail the fool handed himan ordinary cup that had been beside his bed all alongand the king asks how can this behow could you find what all my knights and wisest men could not find and the fool answers i dont know i only knew you were thirsty and for the first time since he was a boy the king felt more than a man not because he was touched by gods glorybut rather by the compassion of a fool']\n",
      "\n",
      "longest conversation has 553 words\n",
      "filetered 25596 conversations\n"
     ]
    }
   ],
   "source": [
    "hist, edges  = np.histogram([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs], density=True, bins=100)\n",
    "center = (edges[:-1] + edges[1:]) / 2\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.bar(center, hist, align='center', width=(edges[1] - edges[0]) * .8)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "longest_converastion = conversations_pairs[np.array([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs]).argmax()]\n",
    "print(\"longest conversation: \\n{}\".format(longest_converastion))\n",
    "print(\"\\nlongest conversation has {} words\".format(len(longest_converastion[0].split(' ')) + len(longest_converastion[1].split(' '))))\n",
    "max_conversation_lenght = 38  # maximum alowed converastion lenght in words\n",
    "clensed_conversations = np.array([conversation_pair for conversation_pair in conversations_pairs  if (len(conversation_pair[0].split(' ')) + len(conversation_pair[1].split(' '))) < max_conversation_lenght])\n",
    "print(\"filetered {} conversations\".format(len(conversations_pairs) - len(clensed_conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shity magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence lenght: 36 words\n"
     ]
    }
   ],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    return np.array(\n",
    "        [to_categorical(sequence, num_classes=vocab_size) for sequence in sequences]\n",
    "    ).reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\n",
    "all_lines = clensed_conversations.reshape(-1)\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.fit_on_texts(all_lines)\n",
    "vocab_size = len(my_tokenizer.word_index) + 1\n",
    "max_sentence_lenght = max(len(line.split()) for line in all_lines)\n",
    "print('max sentence lenght: {} words'.format(max_sentence_lenght))\n",
    "\n",
    "split_index = int(len(clensed_conversations) * .8)\n",
    "train = clensed_conversations[:split_index]\n",
    "test = clensed_conversations[split_index:]\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(my_tokenizer, max_sentence_lenght, train[:, 1])\n",
    "trainY = encode_sequences(my_tokenizer, max_sentence_lenght, train[:, 0])\n",
    "trainY = encode_output(trainY, vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(my_tokenizer, max_sentence_lenght, test[:, 1])\n",
    "testY = encode_sequences(my_tokenizer, max_sentence_lenght, test[:, 0])\n",
    "testY = encode_output(testY, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 397, 256)          6912      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 397, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 397, 256)          525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 397, 27)           6939      \n",
      "=================================================================\n",
      "Total params: 1,064,475\n",
      "Trainable params: 1,064,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "n_units=256\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_tokens, n_units, input_length=max_sentence_lenght, mask_zero=True))\n",
    "model.add(LSTM(n_units))  # CuDNNLSTM\n",
    "model.add(RepeatVector(max_sentence_lenght))\n",
    "model.add(LSTM(n_units, return_sequences=True))  # CuDNNLSTM\n",
    "model.add(TimeDistributed(Dense(num_tokens, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'mount-this/model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('chatbot-seq-400.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you -> fi e\n"
     ]
    }
   ],
   "source": [
    "to_infer = 'how are you'\n",
    "source = np.zeros(max_sentence_lenght)\n",
    "\n",
    "for i in range(len(to_infer)):\n",
    "    source[i] = token_index[to_infer[i]]\n",
    "source = source.reshape((1, source.shape[0]))\n",
    "res = model.predict(source)\n",
    "sentence = \"\".join([input_characters[np.argmax(char_indxes)] for char_indxes in res[0]])\n",
    "print(to_infer + \" -> \" + sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
