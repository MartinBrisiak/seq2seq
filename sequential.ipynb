{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.backend import clear_session\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading movie lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "# prepare regex for char filtering\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def clean_sentence(line):\n",
    "    line = line.strip().replace('--', '').replace(\"  \", \" \").replace('\"', \"\")\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    return ' '.join(line)\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_lines.txt', 'r', errors='ignore') as f:\n",
    "    lines_as_list = [row.strip() for row in f.readlines()]\n",
    "\n",
    "\n",
    "lines = {}\n",
    "for line in lines_as_list:\n",
    "    lines[\n",
    "        line.split('+++$+++')[0].strip()\n",
    "    ] = clean_sentence(line.split('+++$+++')[-1])  # clean sentences\n",
    "\n",
    "del lines_as_list\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_conversations.txt', 'r', errors='ignore') as f:\n",
    "    conversations = [row.strip() for row in f.readlines()]\n",
    "\n",
    "# only take id's and convert list as string to list as list\n",
    "conversations = [\n",
    "    conversation.split('+++$+++')[-1].strip().replace('[', '').replace(']', '').replace(\"'\", '').replace(\" \", '').split(',') \n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "pprint({k: lines[k] for k in list(lines)[:10]})\n",
    "print()\n",
    "pprint(conversations[:10])\n",
    "\n",
    "assert len([conversation for conversation in conversations if len(conversation) <=1]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map keys to line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_with_lines = []\n",
    "for conversation in conversations:\n",
    "    conversations_with_lines.append([lines[key] for key in conversation])\n",
    "    \n",
    "pprint(conversations_with_lines[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair those things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_it(my_list):\n",
    "    pairs = []\n",
    "    for i in range(len(my_list) -1):\n",
    "        pairs.append([my_list[i], my_list[i + 1]])\n",
    "    return pairs\n",
    "\n",
    "paired_conversations_agg = [\n",
    "    pair_it(conversation) for conversation in conversations_with_lines\n",
    "]\n",
    "conversations_pairs = np.array([item for sublist in paired_conversations_agg for item in sublist])\n",
    "for i in range(10):\n",
    "    pprint(conversations_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "hist, edges = np.histogram([len(question) + len(answer) for question, answer in conversations_pairs], density=True, bins=100, normed=True)\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"Conversation length distribution\")\n",
    "\n",
    "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
    "show(p)\n",
    "longest_converastion = conversations_pairs[np.array([len(question) + len(answer) for question, answer in conversations_pairs]).argmax()]\n",
    "print(\"longest conversation: \\n{}\".format(longest_converastion))\n",
    "print(\"\\nlongest conversation lenght: {}\".format(len(longest_converastion[0]) + len(longest_converastion[1])))\n",
    "max_conversation_lenght = 500  # maximum alowed converastion lenght in characters\n",
    "# clensed_conversations = np.where((len(conversations_pairs[:,0]) + len(conversations_pairs[:,1])) < max_conversation_lenght)\n",
    "clensed_conversations = np.array([conversation_pair for conversation_pair in conversations_pairs if len(conversation_pair[0]) + len(conversation_pair[1]) < max_conversation_lenght])\n",
    "print(\"filetered {} conversations\".format(len(conversations_pairs) - len(clensed_conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shity magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for input_text, target_text in clensed_conversations:\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_input_tokens = len(input_characters)\n",
    "num_target_tokens = len(target_characters)\n",
    "max_input_seq_length = max([len(input_text) for input_text, target_text in clensed_conversations])\n",
    "max_target_seq_length = max([len(target_text) for input_text, target_text in clensed_conversations])\n",
    "max_sentence_lenght = max(max_input_seq_length, max_target_seq_length)\n",
    "\n",
    "print('Number of unique input tokens:', num_input_tokens)\n",
    "print('Number of unique output tokens:', num_target_tokens)\n",
    "print('Max sequence length for inputs:', max_input_seq_length)\n",
    "print('Max sequence length for outputs:', max_target_seq_length)\n",
    "\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "split_index = int(len(clensed_conversations) * .8)\n",
    "pre_trainX = clensed_conversations[:split_index, 0]\n",
    "pre_trainY = clensed_conversations[:split_index, 1]\n",
    "pre_testX = clensed_conversations[split_index:, 0]\n",
    "pre_testY = clensed_conversations[split_index:, 1]\n",
    "\n",
    "trainX = np.zeros((len(pre_trainX), max_sentence_lenght), dtype='float32')\n",
    "trainY = np.zeros((len(pre_trainY), max_sentence_lenght, num_target_tokens), dtype='float32')\n",
    "testX = np.zeros((len(pre_testX), max_sentence_lenght), dtype='float32')\n",
    "testY = np.zeros((len(pre_testY), max_sentence_lenght, num_target_tokens), dtype='float32')\n",
    "\n",
    "print(\"trainX {} | trainY {} | testX {} | testY {}\".format(trainX.shape, trainY.shape, testX.shape, testY.shape))\n",
    "\n",
    "for i, (input_text) in enumerate(pre_trainX):\n",
    "    for t, char in enumerate(input_text):\n",
    "        trainX[i, t] = target_token_index[char]\n",
    "\n",
    "for i, (target_text) in enumerate(pre_trainY):\n",
    "    for t, char in enumerate(target_text):\n",
    "        trainY[i, t, target_token_index[char]] = 1.\n",
    "\n",
    "for i, (input_text) in enumerate(pre_testX):\n",
    "    for t, char in enumerate(input_text):\n",
    "        testX[i, t] = target_token_index[char]\n",
    "\n",
    "for i, (target_text) in enumerate(pre_testY):\n",
    "    for t, char in enumerate(target_text):\n",
    "        testY[i, t, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "n_units=256\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_input_tokens, n_units, input_length=max_sentence_lenght, mask_zero=True))\n",
    "model.add(LSTM(n_units))  # CuDNNLSTM\n",
    "model.add(RepeatVector(max_sentence_lenght))\n",
    "model.add(LSTM(n_units, return_sequences=True))  # CuDNNLSTM\n",
    "model.add(TimeDistributed(Dense(num_target_tokens, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'mount-this/model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
