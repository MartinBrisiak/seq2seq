{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.backend import clear_session\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading movie lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "# prepare regex for char filtering\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def clean_sentence(line):\n",
    "    line = line.strip().replace('--', '').replace(\"  \", \" \").replace('\"', \"\")\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    return ' '.join(line)\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_lines.txt', 'r', errors='ignore') as f:\n",
    "    lines_as_list = [row.strip() for row in f.readlines()]\n",
    "\n",
    "\n",
    "lines = {}\n",
    "for line in lines_as_list:\n",
    "    lines[\n",
    "        line.split('+++$+++')[0].strip()\n",
    "    ] = clean_sentence(line.split('+++$+++')[-1])  # clean sentences\n",
    "\n",
    "del lines_as_list\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_conversations.txt', 'r', errors='ignore') as f:\n",
    "    conversations = [row.strip() for row in f.readlines()]\n",
    "\n",
    "# only take id's and convert list as string to list as list\n",
    "conversations = [\n",
    "    conversation.split('+++$+++')[-1].strip().replace('[', '').replace(']', '').replace(\"'\", '').replace(\" \", '').split(',') \n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "pprint({k: lines[k] for k in list(lines)[:10]})\n",
    "print()\n",
    "pprint(conversations[:10])\n",
    "\n",
    "assert len([conversation for conversation in conversations if len(conversation) <=1]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map keys to line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_with_lines = []\n",
    "for conversation in conversations:\n",
    "    conversations_with_lines.append([lines[key] for key in conversation])\n",
    "    \n",
    "pprint(conversations_with_lines[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair those things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_it(my_list):\n",
    "    pairs = []\n",
    "    for i in range(len(my_list) -1):\n",
    "        pairs.append([my_list[i], my_list[i + 1]])\n",
    "    return pairs\n",
    "\n",
    "paired_conversations_agg = [\n",
    "    pair_it(conversation) for conversation in conversations_with_lines\n",
    "]\n",
    "conversations_pairs = np.array([item for sublist in paired_conversations_agg for item in sublist])\n",
    "for i in range(10):\n",
    "    pprint(conversations_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges  = np.histogram([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs], density=True, bins=100)\n",
    "center = (edges[:-1] + edges[1:]) / 2\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.bar(center, hist, align='center', width=(edges[1] - edges[0]) * .8)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "longest_converastion = conversations_pairs[np.array([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs]).argmax()]\n",
    "print(\"longest conversation: \\n{}\".format(longest_converastion))\n",
    "print(\"\\nlongest conversation has {} words\".format(len(longest_converastion[0].split(' ')) + len(longest_converastion[1].split(' '))))\n",
    "max_conversation_lenght = 38  # maximum alowed converastion lenght in words\n",
    "clensed_conversations = np.array([conversation_pair for conversation_pair in conversations_pairs  if (len(conversation_pair[0].split(' ')) + len(conversation_pair[1].split(' '))) < max_conversation_lenght])\n",
    "print(\"filetered {} conversations\".format(len(conversations_pairs) - len(clensed_conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shity magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "all_lines = clensed_conversations.reshape(-1)\n",
    "for line in all_lines:\n",
    "    for word in line.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.add(word)\n",
    "            \n",
    "vocabulary = np.array(sorted(vocabulary))\n",
    "max_sentence_lenght = max(len(line.split()) for line in all_lines)\n",
    "print(\"first word: {}, last word: {}\".format(vocabulary[0], vocabulary[-1]))\n",
    "print('max sentence lenght: {} words'.format(max_sentence_lenght))\n",
    "print('vocab_size: {} words'.format(len(vocabulary)))\n",
    "\n",
    "split_index = int(len(clensed_conversations) * .8)\n",
    "\n",
    "trainX = np.zeros((len(clensed_conversations[:split_index]), max_sentence_lenght), dtype='uint16')\n",
    "trainY = np.zeros((len(clensed_conversations[:split_index]), max_sentence_lenght, vocab_size), dtype='uint8')\n",
    "testX = np.zeros((len(clensed_conversations[split_index:]), max_sentence_lenght), dtype='uint16')\n",
    "testY = np.zeros((len(clensed_conversations[split_index:]), max_sentence_lenght, vocab_size), dtype='uint8')\n",
    "\n",
    "print(\"trainX steps {}\".format(len(trainX)))\n",
    "for i, (sentence) in enumerate(clensed_conversations[:split_index, 0]):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j, (word) in enumerate(sentence.split()):\n",
    "        trainX[i, j] = np.where(vocabulary == word)[0][0]\n",
    "        \n",
    "print(\"trainY steps {}\".format(len(trainY)))\n",
    "for i, (sentence) in enumerate(clensed_conversations[:split_index, 1]):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j, (word) in enumerate(sentence.split()):\n",
    "        index_of_word = np.where(vocabulary == word)[0][0]\n",
    "        trainY[i, j, index_of_word] = 1\n",
    "        \n",
    "print(\"testX steps {}\".format(len(testX)))\n",
    "for i, (sentence) in enumerate(clensed_conversations[split_index:, 0]):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j, (word) in enumerate(sentence.split()):\n",
    "        testX[i, j] = np.where(vocabulary == word)[0][0]\n",
    "        \n",
    "print(\"testY steps {}\".format(len(testY)))\n",
    "for i, (sentence) in enumerate(clensed_conversations[split_index:, 1]):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j, (word) in enumerate(sentence.split()):\n",
    "        index_of_word = np.where(vocabulary == word)[0][0]\n",
    "        testY[i, j, index_of_word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "n_units=256\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary) + 1, n_units, input_length=max_sentence_lenght, mask_zero=True))\n",
    "model.add(LSTM(n_units))  # CuDNNLSTM\n",
    "model.add(RepeatVector(max_sentence_lenght))\n",
    "model.add(LSTM(n_units, return_sequences=True))  # CuDNNLSTM\n",
    "model.add(TimeDistributed(Dense(len(vocabulary) + 1, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'mount-this/model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('chatbot-seq-400.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_infer = 'how are you'\n",
    "# source = np.zeros(max_sentence_lenght)\n",
    "\n",
    "# for i in range(len(to_infer)):\n",
    "#     source[i] = token_index[to_infer[i]]\n",
    "# source = source.reshape((1, source.shape[0]))\n",
    "# res = model.predict(source)\n",
    "# sentence = \"\".join([input_characters[np.argmax(char_indxes)] for char_indxes in res[0]])\n",
    "# print(to_infer + \" -> \" + sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
