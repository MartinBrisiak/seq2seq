{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "# import plaidml.keras.backend as K\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Concatenate, Input, Embedding, TimeDistributed, Flatten, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading movie lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "# prepare regex for char filtering\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def clean_sentence(line):\n",
    "    line = line.strip().replace('--', '').replace(\"  \", \" \").replace('\"', \"\")\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    return ' '.join(line)\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_lines.txt', 'r', errors='ignore') as f:\n",
    "    lines_as_list = [row.strip() for row in f.readlines()]\n",
    "\n",
    "\n",
    "lines = {}\n",
    "for line in lines_as_list:\n",
    "    lines[\n",
    "        line.split('+++$+++')[0].strip()\n",
    "    ] = clean_sentence(line.split('+++$+++')[-1])  # clean sentences\n",
    "\n",
    "del lines_as_list\n",
    "\n",
    "with open('./cornell-movie-dialogs-corpus/movie_conversations.txt', 'r', errors='ignore') as f:\n",
    "    conversations = [row.strip() for row in f.readlines()]\n",
    "\n",
    "# only take id's and convert list as string to list as list\n",
    "conversations = [\n",
    "    conversation.split('+++$+++')[-1].strip().replace('[', '').replace(']', '').replace(\"'\", '').replace(\" \", '').split(',') \n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "pprint({k: lines[k] for k in list(lines)[:10]})\n",
    "print()\n",
    "pprint(conversations[:10])\n",
    "\n",
    "assert len([conversation for conversation in conversations if len(conversation) <=1]) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map keys to line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_with_lines = []\n",
    "for conversation in conversations:\n",
    "    conversations_with_lines.append([lines[key] for key in conversation])\n",
    "    \n",
    "pprint(conversations_with_lines[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair those things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_it(my_list):\n",
    "    pairs = []\n",
    "    for i in range(len(my_list) -1):\n",
    "        pairs.append([my_list[i], my_list[i + 1]])\n",
    "    return pairs\n",
    "\n",
    "paired_conversations_agg = [\n",
    "    pair_it(conversation) for conversation in conversations_with_lines\n",
    "]\n",
    "conversations_pairs = np.array([item for sublist in paired_conversations_agg for item in sublist])\n",
    "for i in range(10):\n",
    "    pprint(conversations_pairs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges  = np.histogram([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs], density=True, bins=100)\n",
    "center = (edges[:-1] + edges[1:]) / 2\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "plt.xlabel('all conversations', fontsize=14)\n",
    "plt.bar(center, hist, align='center', width=(edges[1] - edges[0]) * .8)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "longest_converastion = conversations_pairs[np.array([len(question.split(' ')) + len(answer.split(' ')) for question, answer in conversations_pairs]).argmax()]\n",
    "print(\"longest conversation: \\n{}\\n\".format(longest_converastion))\n",
    "print(\"longest conversation has {} words.\".format(len(longest_converastion[0].split(' ')) + len(longest_converastion[1].split(' '))))\n",
    "max_sentence_lenght = 10  # maximum alowed converastion lenght in words\n",
    "clensed_conversations = np.array([conversation_pair for conversation_pair in conversations_pairs  if len(conversation_pair[0].split(' ')) < max_sentence_lenght and len(conversation_pair[1].split(' '))  < max_sentence_lenght ])\n",
    "print(\"filetered {} conversations\\n\".format(len(conversations_pairs) - len(clensed_conversations)))\n",
    "\n",
    "hist, edges  = np.histogram([len(question.split(' ')) + len(answer.split(' ')) for question, answer in clensed_conversations], density=True, bins=100)\n",
    "center = (edges[:-1] + edges[1:]) / 2\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "plt.bar(center, hist, align='center', width=(edges[1] - edges[0]) * .8)\n",
    "plt.xlabel('reduced conversations', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "longest_converastion = clensed_conversations[np.array([len(question.split(' ')) + len(answer.split(' ')) for question, answer in clensed_conversations]).argmax()]\n",
    "print(\"longest conversation in reduce dataset: \\n{}\\n\".format(longest_converastion))\n",
    "print(\"longest conversation in reduce dataset has {} words.\".format(len(longest_converastion[0].split(' ')) + len(longest_converastion[1].split(' '))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shity magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clensed_conversations.reshape(-1))\n",
    "vocabulary = np.fromiter(tokenizer.word_index.keys(), dtype=\"<U34\")\n",
    "all_lines = clensed_conversations.reshape(-1)\n",
    "\n",
    "max_sentence_lenght = max(len(line.split()) for line in all_lines)\n",
    "print(\"first word: {}, last word: {}\".format(vocabulary[0], vocabulary[-1]))\n",
    "print('max sentence lenght: {} words'.format(max_sentence_lenght))\n",
    "print('vocab_size: {} words'.format(len(vocabulary)))\n",
    "\n",
    "encoder_input_data = np.zeros((len(clensed_conversations), max_sentence_lenght, len(vocabulary)), dtype='uint8')\n",
    "decoder_input_data = np.zeros((len(clensed_conversations), max_sentence_lenght, len(vocabulary)), dtype='uint8')\n",
    "decoder_target_data = np.zeros((len(clensed_conversations), max_sentence_lenght, len(vocabulary)), dtype='uint8')\n",
    "\n",
    "with tqdm_notebook(total=len(clensed_conversations)) as pbar:\n",
    "    for i, (left, right) in enumerate(clensed_conversations):\n",
    "        for t, (word) in enumerate(left.split()):\n",
    "            encoder_input_data[i, t, np.where(vocabulary == word)[0][0]] = 1\n",
    "        for t, (word) in enumerate(right.split()):\n",
    "            decoder_input_data[i, t, np.where(vocabulary == word)[0][0]] = 1\n",
    "            if t > 0:\n",
    "                decoder_target_data[i, t-1, np.where(vocabulary == word)[0][0]] = 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "![sequential](./images/seq2seq.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "latent_dim = 1024\n",
    "\n",
    "encoder_inputs = Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "e_outputs, h1, c1 = LSTM(latent_dim, return_state=True, return_sequences=True)(encoder_inputs) \n",
    "_, h2, c2 = LSTM(latent_dim, return_state=True)(e_outputs) \n",
    "encoder_states = [h1, c1, h2, c2]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, len(vocabulary)))\n",
    "\n",
    "out_layer1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "d_outputs, dh1, dc1 = out_layer1(decoder_inputs,initial_state= [h1, c1])\n",
    "out_layer2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "final, dh2, dc2 = out_layer2(d_outputs, initial_state= [h2, c2])\n",
    "decoder_dense = Dense(len(vocabulary), activation='softmax')\n",
    "decoder_outputs = decoder_dense(final)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"mount-this/seq2seq-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "filename = 'mount-this/seq2seq-model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, verbose=1, save_best_only=True)\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[checkpoint],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"mount-this/seq2seq-model.h5\")\n",
    "# # model = load_model(\"mount-this/seq2seq-overfited-model.h5\")\n",
    "\n",
    "# encoder_inputs = model.input[0]   # input_1\n",
    "# encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n",
    "# encoder_states = [state_h_enc, state_c_enc]\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# decoder_inputs = model.input[1]   # input_2\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_lstm = model.layers[3]\n",
    "# decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_h_dec, state_c_dec]\n",
    "# decoder_dense = model.layers[4]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,\n",
    "#     [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_infer = \"hello\"\n",
    "encoded_infer = np.zeros((max_sentence_lenght, len(vocabulary)), dtype='uint8')\n",
    "# decoded_infer = np.zeros((max_sentence_lenght, len(vocabulary)), dtype='uint8')\n",
    "for i in range(len(to_infer.split())):\n",
    "    encoded_infer[i, np.where(vocabulary == to_infer.split()[i])[0][0]] = 1\n",
    "encoded_infer = encoded_infer.reshape((1, encoded_infer.shape[0], encoded_infer.shape[1]))\n",
    "\n",
    "res = encoder_model.predict(encoded_infer)\n",
    "sentence = \" \".join([vocabulary[np.argmax(word_indexes)] for word_indexes in res[0] if np.max(word_indexes) > .2])\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
